{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "Using a GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "if tf.test.gpu_device_name() == '/device:GPU:0':\n",
    "  print(\"Using a GPU\")\n",
    "else:\n",
    "  print(\"Using a CPU\")\n",
    "\n",
    "import os\n",
    "#print(os.environ['HOME'])\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import functools\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.style.use('ggplot')\n",
    "import numpy as np\n",
    "import seaborn as sns; sns.set_context('notebook')\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Utils { display-mode: \"form\" }\n",
    "def print_subclasses_from_module(module, base_class, maxwidth=80):\n",
    "  import functools, inspect, sys\n",
    "  subclasses = [name for name, obj in inspect.getmembers(module)\n",
    "                if inspect.isclass(obj) and issubclass(obj, base_class)]\n",
    "  def red(acc, x):\n",
    "    if not acc or len(acc[-1]) + len(x) + 2 > maxwidth:\n",
    "      acc.append(x)\n",
    "    else:\n",
    "      acc[-1] += \", \" + x\n",
    "    return acc\n",
    "  print('\\n'.join(functools.reduce(red, subclasses, [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mats = tf.random.uniform(shape=[1000, 10, 10])\n",
    "vecs = tf.random.uniform(shape=[1000, 10, 1])\n",
    "\n",
    "def for_loop_solve():\n",
    "  return np.array(\n",
    "    [tf.linalg.solve(mats[i, ...], vecs[i, ...]) for i in range(1000)])\n",
    "\n",
    "def vectorized_solve():\n",
    "  return tf.linalg.solve(mats, vecs)\n",
    "\n",
    "# Vectorization for the win!\n",
    "%timeit for_loop_solve()\n",
    "%timeit vectorized_solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(np.pi)\n",
    "b = tf.constant(np.e)\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch([a, b])\n",
    "  c = .5 * (a**2 + b**2)\n",
    "grads = tape.gradient(c, [a, b])\n",
    "print(grads[0])\n",
    "print(grads[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_subclasses_from_module(tfp.distributions, tfp.distributions.Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A standard normal\n",
    "normal = tfd.Normal(loc=0., scale=1.)\n",
    "print(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1000 samples from a standard normal\n",
    "samples = normal.sample(1000)\n",
    "sns.distplot(samples)\n",
    "plt.title(\"Samples from a standard Normal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the log_prob of a point in the event space of `normal`\n",
    "normal.log_prob(0.)\n",
    "# Compute the log_prob of a few points\n",
    "normal.log_prob([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of 3 normals, and plot 1000 samples from each\n",
    "normals = tfd.Normal([-2.5, 0., 2.5], 1.)  # The scale parameter broadacasts!\n",
    "print(\"Batch shape:\", normals.batch_shape)\n",
    "print(\"Event shape:\", normals.event_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample shapes can themselves be more complicated\n",
    "print(\"Shape of samples:\", normals.sample([10, 10, 10]).shape)\n",
    "# A batch of normals gives a batch of log_probs.\n",
    "print(normals.log_prob([-2.5, 0., 2.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal numpy-like broadcasting rules apply!\n",
    "xs = np.linspace(-6, 6, 200)\n",
    "try:\n",
    "  normals.log_prob(xs)\n",
    "except Exception as e:\n",
    "  print(\"TFP error:\", e.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But this would work:\n",
    "a = np.zeros([200, 1]) + np.zeros(3)\n",
    "print(\"Broadcast shape:\", a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And so will this!\n",
    "xs = np.linspace(-6, 6, 200)[..., np.newaxis]\n",
    "# => shape = [200, 1]\n",
    "\n",
    "lps = normals.log_prob(xs)\n",
    "print(\"Broadcast log_prob shape:\", lps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing visually\n",
    "samples = normals.sample(1000)\n",
    "for i in range(3):\n",
    "  sns.distplot(samples[:, i], kde=False, norm_hist=True)\n",
    "plt.plot(np.tile(xs, 3), normals.prob(xs), c='k', alpha=.5)\n",
    "plt.title(\"Samples from 3 Normals, and their PDF's\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "def f(x, w):\n",
    "  # Pad x with 1's so we can add bias via matmul\n",
    "  x = tf.pad(x, [[1, 0], [0, 0]], constant_values=1)\n",
    "  linop = tf.linalg.LinearOperatorFullMatrix(w[..., np.newaxis])\n",
    "  result = linop.matmul(x, adjoint=True)\n",
    "  return result[..., 0, :]\n",
    "\n",
    "num_features = 2\n",
    "num_examples = 50\n",
    "noise_scale = .5\n",
    "true_w = np.array([-1., 2., 3.])\n",
    "\n",
    "xs = np.random.uniform(-1., 1., [num_features, num_examples])\n",
    "ys = f(xs, true_w) + np.random.normal(0., noise_scale, size=num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data set\n",
    "plt.scatter(*xs, c=ys, s=100, linewidths=0)\n",
    "\n",
    "grid = np.meshgrid(*([np.linspace(-1, 1, 100)] * 2))\n",
    "xs_grid = np.stack(grid, axis=0)\n",
    "fs_grid = f(xs_grid.reshape([num_features, -1]), true_w)\n",
    "fs_grid = np.reshape(fs_grid, [100, 100])\n",
    "plt.colorbar()\n",
    "plt.contour(xs_grid[0, ...], xs_grid[1, ...], fs_grid, 20, linewidths=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the joint_log_prob function, and our unnormalized posterior.\n",
    "def joint_log_prob(w, x, y):\n",
    "  # Our model in maths is\n",
    "  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))\n",
    "  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N\n",
    "\n",
    "  rv_w = tfd.MultivariateNormalDiag(\n",
    "    loc=np.zeros(num_features + 1),\n",
    "    scale_diag=np.ones(num_features + 1))\n",
    "\n",
    "  rv_y = tfd.Normal(f(x, w), noise_scale)\n",
    "  return (rv_w.log_prob(w) +\n",
    "          tf.reduce_sum(rv_y.log_prob(y), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our unnormalized target density by currying x and y from the joint.\n",
    "def unnormalized_posterior(w):\n",
    "  return joint_log_prob(w, xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an HMC TransitionKernel\n",
    "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "  target_log_prob_fn=unnormalized_posterior,\n",
    "  step_size=np.float64(.1),\n",
    "  num_leapfrog_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We wrap sample_chain in tf.function, telling TF to precompile a reusable\n",
    "# computation graph, which will dramatically improve performance.\n",
    "@tf.function\n",
    "def run_chain(initial_state, num_results=10000, num_burnin_steps=5000):\n",
    "  return tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    current_state=initial_state,\n",
    "    kernel=hmc_kernel,\n",
    "    trace_fn=lambda current_state, kernel_results: kernel_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = np.zeros(num_features + 1)\n",
    "samples, kernel_results = run_chain(initial_state)\n",
    "print(\"Acceptance rate:\", kernel_results.is_accepted.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a simple step size adaptation during burnin\n",
    "@tf.function\n",
    "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\n",
    "  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "      hmc_kernel,\n",
    "      num_adaptation_steps=int(.8 * num_burnin_steps),\n",
    "      target_accept_prob=np.float64(.65))\n",
    "\n",
    "  return tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    current_state=initial_state,\n",
    "    kernel=adaptive_kernel,\n",
    "    trace_fn=lambda cs, kr: kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, kernel_results = run_chain(\n",
    "  initial_state=np.zeros(num_features+1))\n",
    "print(\"Acceptance rate:\", kernel_results.inner_results.is_accepted.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots\n",
    "colors = ['b', 'g', 'r']\n",
    "for i in range(3):\n",
    "  plt.plot(samples[:, i], c=colors[i], alpha=.3)\n",
    "  plt.hlines(true_w[i], 0, 1000, zorder=4, color=colors[i], label=\"$w_{}$\".format(i))\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Histogram of samples\n",
    "for i in range(3):\n",
    "  sns.distplot(samples[:, i], color=colors[i])\n",
    "ymax = plt.ylim()[1]\n",
    "for i in range(3):\n",
    "  plt.vlines(true_w[i], 0, ymax, color=colors[i])\n",
    "plt.ylim(0, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a single set of initial w's, we create a batch of 8.\n",
    "num_chains = 8\n",
    "initial_state = np.zeros([num_chains, num_features + 1])\n",
    "\n",
    "chains, kernel_results = run_chain(initial_state)\n",
    "\n",
    "r_hat = tfp.mcmc.potential_scale_reduction(chains)\n",
    "print(\"Acceptance rate:\", kernel_results.inner_results.is_accepted.numpy().mean())\n",
    "print(\"R-hat diagnostic (per latent variable):\", r_hat.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the joint_log_prob function, and our unnormalized posterior.\n",
    "def joint_log_prob(w, sigma, x, y):\n",
    "  # Our model in maths is\n",
    "  #   w ~ MVN([0, 0, 0], diag([1, 1, 1]))\n",
    "  #   y_i ~ Normal(w @ x_i, noise_scale),  i=1..N\n",
    "\n",
    "  rv_w = tfd.MultivariateNormalDiag(\n",
    "    loc=np.zeros(num_features + 1),\n",
    "    scale_diag=np.ones(num_features + 1))\n",
    "  \n",
    "  rv_sigma = tfd.LogNormal(np.float64(1.), np.float64(5.))\n",
    "\n",
    "  rv_y = tfd.Normal(f(x, w), sigma[..., np.newaxis])\n",
    "  return (rv_w.log_prob(w) +\n",
    "          rv_sigma.log_prob(sigma) +\n",
    "          tf.reduce_sum(rv_y.log_prob(y), axis=-1))\n",
    "\n",
    "# Create our unnormalized target density by currying x and y from the joint.\n",
    "def unnormalized_posterior(w, sigma):\n",
    "  return joint_log_prob(w, sigma, xs, ys)\n",
    "\n",
    "\n",
    "# Create an HMC TransitionKernel\n",
    "hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "  target_log_prob_fn=unnormalized_posterior,\n",
    "  step_size=np.float64(.1),\n",
    "  num_leapfrog_steps=4)\n",
    "\n",
    "\n",
    "\n",
    "# Create a TransformedTransitionKernl\n",
    "transformed_kernel = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=hmc_kernel,\n",
    "    bijector=[tfb.Identity(),    # w\n",
    "              tfb.Invert(tfb.Softplus())])   # sigma\n",
    "\n",
    "\n",
    "# Apply a simple step size adaptation during burnin\n",
    "@tf.function\n",
    "def run_chain(initial_state, num_results=1000, num_burnin_steps=500):\n",
    "  adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "      transformed_kernel,\n",
    "      num_adaptation_steps=int(.8 * num_burnin_steps),\n",
    "      target_accept_prob=np.float64(.75))\n",
    "\n",
    "  return tfp.mcmc.sample_chain(\n",
    "    num_results=num_results,\n",
    "    num_burnin_steps=num_burnin_steps,\n",
    "    current_state=initial_state,\n",
    "    kernel=adaptive_kernel,\n",
    "    trace_fn=lambda cs, kr: kr)\n",
    "\n",
    "\n",
    "# Instead of a single set of initial w's, we create a batch of 8.\n",
    "num_chains = 8\n",
    "initial_state = [np.zeros([num_chains, num_features + 1]),\n",
    "                 .54 * np.ones([num_chains], dtype=np.float64)]\n",
    "\n",
    "chains, kernel_results = run_chain(initial_state)\n",
    "\n",
    "r_hat = tfp.mcmc.potential_scale_reduction(chains)\n",
    "print(\"Acceptance rate:\", kernel_results.inner_results.inner_results.is_accepted.numpy().mean())\n",
    "print(\"R-hat diagnostic (per w variable):\", r_hat[0].numpy())\n",
    "print(\"R-hat diagnostic (sigma):\", r_hat[1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_chains, sigma_chains = chains\n",
    "\n",
    "# Trace plots of w (one of 8 chains)\n",
    "colors = ['b', 'g', 'r', 'teal']\n",
    "fig, axes = plt.subplots(4, num_chains, figsize=(4 * num_chains, 8))\n",
    "for j in range(num_chains):\n",
    "  for i in range(3):\n",
    "    ax = axes[i][j]\n",
    "    ax.plot(w_chains[:, j, i], c=colors[i], alpha=.3)\n",
    "    ax.hlines(true_w[i], 0, 1000, zorder=4, color=colors[i], label=\"$w_{}$\".format(i))\n",
    "    ax.legend(loc='upper right')\n",
    "  ax = axes[3][j]\n",
    "  ax.plot(sigma_chains[:, j], alpha=.3, c=colors[3])\n",
    "  ax.hlines(noise_scale, 0, 1000, zorder=4, color=colors[3], label=\"$\\sigma$\".format(i))\n",
    "  ax.legend(loc='upper right')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Histogram of samples of w\n",
    "fig, axes = plt.subplots(4, num_chains, figsize=(4 * num_chains, 8))\n",
    "for j in range(num_chains):\n",
    "  for i in range(3):\n",
    "    ax = axes[i][j]\n",
    "    sns.distplot(w_chains[:, j, i], color=colors[i], norm_hist=True, ax=ax, hist_kws={'alpha': .3})\n",
    "  for i in range(3):\n",
    "    ax = axes[i][j]\n",
    "    ymax = ax.get_ylim()[1]\n",
    "    ax.vlines(true_w[i], 0, ymax, color=colors[i], label=\"$w_{}$\".format(i), linewidth=3)\n",
    "    ax.set_ylim(0, ymax)\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "\n",
    "  ax = axes[3][j]\n",
    "  sns.distplot(sigma_chains[:, j], color=colors[3], norm_hist=True, ax=ax, hist_kws={'alpha': .3})\n",
    "  ymax = ax.get_ylim()[1]\n",
    "  ax.vlines(noise_scale, 0, ymax, color=colors[3], label=\"$\\sigma$\".format(i), linewidth=3)\n",
    "  ax.set_ylim(0, ymax)\n",
    "  ax.legend(loc='upper right')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf-prob] *",
   "language": "python",
   "name": "conda-env-.conda-tf-prob-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
